{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac716d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "/home/nthadishetty1/frame_interpollation/plucker.py:58: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /pytorch/aten/src/ATen/native/Cross.cpp:63.)\n",
      "  rays_dxo = torch.cross(rays_o, rays_d)                          # B, V, HW, 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Video: stereo_vkitti200000001\n",
      "Current Video: stereo_vkitti200000006\n",
      "Current Video: stereo_vkitti200000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nthadishetty1/frame_interpollation/plucker.py:58: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /pytorch/aten/src/ATen/native/Cross.cpp:63.)\n",
      "  rays_dxo = torch.cross(rays_o, rays_d)                          # B, V, HW, 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46]), first_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46])\n",
      " latents shape is torch.Size([1, 15, 4, 46, 46])\n",
      "last_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46]), first_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46])\n",
      " latents shape is torch.Size([1, 15, 4, 46, 46])\n",
      "last_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46]), first_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46])\n",
      " latents shape is torch.Size([1, 15, 4, 46, 46])\n",
      "last_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46]), first_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46])\n",
      " latents shape is torch.Size([1, 15, 4, 46, 46])\n",
      "last_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46]), first_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46])\n",
      " latents shape is torch.Size([1, 15, 4, 46, 46])\n",
      "last_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46]), first_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46])\n",
      " latents shape is torch.Size([1, 15, 4, 46, 46])\n",
      "last_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46]), first_left_rgb_latent shape, after latent conversion is torch.Size([1, 15, 4, 46, 46])\n",
      " latents shape is torch.Size([1, 15, 4, 46, 46])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from natsort import natsorted\n",
    "import cv2\n",
    "from typing import Dict, Any, List\n",
    "import random, json\n",
    "from plucker import ray_condition, RandomHorizontalFlipWithPose \n",
    "import diffusers\n",
    "from diffusers import AutoencoderKLTemporalDecoder\n",
    "from einops import rearrange\n",
    "\n",
    "class StereoEventDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, video_data_dir,\n",
    "        frame_height=375, frame_width=375,\n",
    "        random_seed=42):\n",
    "        self.video_data_dir = video_data_dir\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        video_names = sorted([v for v in os.listdir(video_data_dir)if os.path.isdir(os.path.join(video_data_dir, v))])\n",
    "        self.video_names = video_names\n",
    "        self.length = len(self.video_names)\n",
    "\n",
    "        self.transform_rgb = transforms.Compose([transforms.Resize((frame_height, frame_width),interpolation=transforms.InterpolationMode.BILINEAR),transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "        self.transforms_evs = transforms.Compose([transforms.Normalize(mean=[0.5] * 6, std=[0.5] * 6, inplace=True)])\n",
    "        self.pixel_transforms = [transforms.Resize((375, 375)),RandomHorizontalFlipWithPose(),transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)]\n",
    "        self.flip_flag = self.pixel_transforms[1].get_flip_flag(17)\n",
    "\n",
    "    @staticmethod\n",
    "    def great_filter(event_image: np.ndarray, kernel_size: int = 3, iterations: int = 1) -> np.ndarray:\n",
    "        event_image = event_image.astype(np.float32)\n",
    "        max_val = np.max(event_image)\n",
    "        if max_val > 0:\n",
    "            event_image = event_image / max_val\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "        dilated = cv2.dilate(event_image, kernel, iterations=iterations)\n",
    "        eroded = cv2.erode(dilated, kernel, iterations=iterations)\n",
    "        eroded = np.clip(eroded, 0, 1)\n",
    "        return eroded\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_function(event_image, kernel_size=31, kernel_size_erode=61,kernel_size_midele=31, iterations=1, sigma_log=10):\n",
    "        max_value = np.max(np.abs(event_image))\n",
    "        if max_value != 0:\n",
    "            event_image = np.abs(event_image) / max_value\n",
    "        else:\n",
    "            event_image = np.abs(event_image)\n",
    "\n",
    "        event_image_blurred = cv2.GaussianBlur(event_image, (kernel_size,kernel_size), sigma_log)\n",
    "        _, binary_image = cv2.threshold(event_image_blurred, 0.01, 1, cv2.THRESH_BINARY)    \n",
    "        kernel_dilate = np.ones((kernel_size_erode, kernel_size_erode), np.uint8)\n",
    "        binary_image_dilated = cv2.dilate(binary_image, kernel_dilate, iterations=iterations )\n",
    "        binary_median = cv2.medianBlur(binary_image_dilated.astype(np.uint8), kernel_size_midele)\n",
    "        return binary_median\n",
    "\n",
    "    def _load_image(self, image_path: str, channels: int = 3):\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)    \n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        return img\n",
    "    \n",
    "    def get_video_paths(self, video_names):  \n",
    "        paths = {}\n",
    "        for cam, folder in zip(['left', 'right'], ['camera_00_rgb_depth_events', 'camera_01_rgb_depth_events']):\n",
    "            cam_path = os.path.join(self.video_data_dir, video_names, folder)\n",
    "            rgb = natsorted(glob(os.path.join(cam_path, 'images', '*.png')))\n",
    "            depth = natsorted(glob(os.path.join(cam_path, 'depths', '*.npy')))\n",
    "            event = natsorted(glob(os.path.join(cam_path, 'events', '*.png')))\n",
    "            meta_file_path = os.path.join(cam_path, 'metadata.json')\n",
    "            camera_metadata = {}\n",
    "            with open(meta_file_path, 'r') as f:\n",
    "                camera_metadata = json.load(f)\n",
    "            paths[cam] = {'rgb': rgb,'depth': depth, 'event': event, 'metadata': camera_metadata,'metadata_path': meta_file_path}\n",
    "        return paths\n",
    "\n",
    "    def _get_paths(self, video_names):\n",
    "        return self.get_video_paths(video_names)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def _load_rgb(self, rgb_paths: List[str]):\n",
    "        frames = [self._load_image(path, channels=3) for path in rgb_paths]\n",
    "        self.num_rgb_frames = len(frames)\n",
    "        rgb_stack = np.stack(frames) \n",
    "        rgb_stack = np.transpose(rgb_stack, (0, 3, 1, 2))\n",
    "        rgb_stack = torch.from_numpy(rgb_stack).float()\n",
    "        rgb_stack = (rgb_stack - torch.min(rgb_stack))/( torch.max(rgb_stack) - torch.min(rgb_stack) )\n",
    "        return rgb_stack\n",
    "\n",
    "    def _load_depth(self, depth_paths: List[str]) -> torch.Tensor:\n",
    "        depth_list = []\n",
    "        for path in depth_paths:\n",
    "            depth_map = np.load(path).astype(np.float32)\n",
    "            depth_resized = cv2.resize(depth_map, (self.frame_width, self.frame_height),interpolation=cv2.INTER_NEAREST)\n",
    "            depth_list.append(depth_resized)\n",
    "        depth_stack = np.stack(depth_list)\n",
    "        depth_stack = np.expand_dims(depth_stack, axis=1) \n",
    "        return torch.from_numpy(depth_stack).float()\n",
    "\n",
    "    def _load_events(self, event_paths, num_timesteps= 16, num_channels= 6):\n",
    "        expected_files = num_timesteps * num_channels \n",
    "        event_frames = []\n",
    "        for t in range(num_timesteps): \n",
    "            channels = []\n",
    "            for ch in range(num_channels):\n",
    "                idx = t * num_channels + ch \n",
    "                \n",
    "                if idx < len(event_paths):\n",
    "                    img = cv2.imread(event_paths[idx], cv2.IMREAD_UNCHANGED) \n",
    "                    filtered = StereoEventDataset.great_filter(img) \n",
    "                    channels.append(filtered)\n",
    "                else:\n",
    "                    channels.append(np.zeros((self.frame_height, self.frame_width), dtype=np.float32))\n",
    "                    \n",
    "            frame = np.stack(channels, axis=-1)\n",
    "            event_frames.append(frame)\n",
    "\n",
    "        event_stack = np.stack(event_frames)\n",
    "        event_stack = np.transpose(event_stack, (0, 3, 1, 2))\n",
    "        event_tensor = torch.from_numpy(event_stack).float()\n",
    "        \n",
    "        blank_frame = torch.zeros((1, num_channels, self.frame_height, self.frame_width),dtype=torch.float32)\n",
    "        event_tensor = torch.cat([blank_frame, event_tensor], dim=0)\n",
    "        return event_tensor\n",
    "\n",
    "\n",
    "    def crop_center_patch(self, pixel_values, event_voxel_bin, crop_h=375, crop_w=375, random_crop=False):\n",
    "        height = pixel_values.shape[2]\n",
    "        width = pixel_values.shape[3]\n",
    "        if random_crop:\n",
    "            start_h = random.randint(0, height - crop_h)\n",
    "            start_w = random.randint(0, width - crop_w)\n",
    "        else:\n",
    "            center_h = height // 2\n",
    "            center_w = width // 2\n",
    "            start_h = center_h - crop_h // 2\n",
    "            start_w = center_w - crop_w // 2\n",
    "        cropped_pixel_values = pixel_values[:, :, start_h:start_h+crop_h, start_w:start_w+crop_w]\n",
    "        cropped_event_voxel_bin = event_voxel_bin[:, :, start_h:start_h+crop_h, start_w:start_w+crop_w]\n",
    "        return cropped_pixel_values, cropped_event_voxel_bin\n",
    "\n",
    "\n",
    "    def plucker_embeddings(self, paths, flip_flag, frame_height=375, frame_width=375):\n",
    "        EXTRINSICS_KEY = 'extrinsics'\n",
    "        INTRINSICS_KEY = 'intrinsics'\n",
    "        embeddings = {}\n",
    "        for cam in ['left', 'right']:\n",
    "            camera_paras = paths[cam]['metadata']\n",
    "            extrinsics_np = np.array(camera_paras[EXTRINSICS_KEY], dtype=np.float32)\n",
    "            c2w_tensor = torch.as_tensor(extrinsics_np).unsqueeze(0)  \n",
    "            K_matrix_np = np.array(camera_paras[INTRINSICS_KEY], dtype=np.float32)\n",
    "            intrinsics_vec_np = np.array([K_matrix_np[0,0], K_matrix_np[1,1], K_matrix_np[0,2], K_matrix_np[1,2]], dtype=np.float32)\n",
    "            intrinsics_tensor = torch.as_tensor(intrinsics_vec_np).unsqueeze(0).repeat(1, 17, 1) \n",
    "            embedding = ray_condition(intrinsics_tensor, c2w_tensor, frame_height, frame_width, device='cpu', flip_flag=flip_flag)[0].permute(0, 3, 1, 2).contiguous()\n",
    "            embeddings[cam] = embedding\n",
    "        return embeddings['left'], embeddings['right']\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        video_name = self.video_names[idx]\n",
    "        paths = self._get_paths(video_name)\n",
    "        left_meta_path = paths['left']['metadata_path']\n",
    "        right_meta_path = paths['right']['metadata_path']\n",
    "        left_rgb = self._load_rgb(paths['left']['rgb'])\n",
    "        left_depth = self._load_depth(paths['left']['depth'])\n",
    "        left_event = self._load_events(paths['left']['event'])\n",
    "\n",
    "        right_rgb = self._load_rgb(paths['right']['rgb'])\n",
    "        right_depth = self._load_depth(paths['right']['depth'])\n",
    "        right_event = self._load_events(paths['right']['event'])\n",
    "\n",
    "        plucker_embedding_left, plucker_embedding_right = self.plucker_embeddings(paths=paths,flip_flag=self.flip_flag,frame_height=self.frame_height,frame_width=self.frame_width)\n",
    "        def apply_transform_to_sequence(sequence_tensor, transform_fn):\n",
    "            if sequence_tensor.ndim == 3:\n",
    "                return transform_fn(sequence_tensor)\n",
    "            transformed_frames = []\n",
    "            for t in range(sequence_tensor.shape[0]):\n",
    "                transformed_frame = transform_fn(sequence_tensor[t])\n",
    "                transformed_frames.append(transformed_frame)\n",
    "            return torch.stack(transformed_frames, dim=0)\n",
    "\n",
    "        left_rgb = apply_transform_to_sequence(left_rgb, self.transform_rgb)\n",
    "        right_rgb = apply_transform_to_sequence(right_rgb, self.transform_rgb)\n",
    "\n",
    "        left_event = apply_transform_to_sequence(left_event, self.transforms_evs)\n",
    "        right_event = apply_transform_to_sequence(right_event, self.transforms_evs)\n",
    "\n",
    "        left_pixel_values, left_events = self.crop_center_patch(pixel_values=left_rgb,event_voxel_bin=left_event,random_crop=True)\n",
    "        right_pixel_values, right_events = self.crop_center_patch(pixel_values=right_rgb,event_voxel_bin=right_event,random_crop=True)\n",
    "\n",
    "        final_frame = left_pixel_values[-1] \n",
    "        sample = dict(left=dict(pixel_values=left_pixel_values,events=left_events,depth=left_depth,plucker_embedding=plucker_embedding_left),\n",
    "            right=dict(pixel_values=right_pixel_values,events=right_events,depth=right_depth,plucker_embedding=plucker_embedding_right), video_name=video_name)\n",
    "        return sample\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = StereoEventDataset(video_data_dir=\"/home/nthadishetty1/frame_interpollation/depth_event_rgd_data\",frame_height=375,frame_width=375)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,shuffle=None,collate_fn=None,batch_size=1, num_workers=1)\n",
    "vae = AutoencoderKLTemporalDecoder.from_pretrained(\"stabilityai/stable-video-diffusion-img2vid-xt\", subfolder=\"vae\", variant='fp16')\n",
    "weight_dtype = torch.float16\n",
    "vae.to(device, dtype=weight_dtype)\n",
    "\n",
    "# for batch_idx, batch in enumerate(train_dataloader):\n",
    "#     video_name = batch['video_name'][0]     \n",
    "#     left_pixel_values_batch = batch['left']['pixel_values'] \n",
    "#     left_events_batch = batch['left']['events']             \n",
    "#     left_depth_batch = batch['left']['depth']               \n",
    "#     left_plucker_embedding_batch = batch['left']['plucker_embedding'] \n",
    "\n",
    "#     right_pixel_values_batch = batch['right']['pixel_values']\n",
    "#     right_events_batch = batch['right']['events']\n",
    "#     right_depth_batch = batch['right']['depth']\n",
    "#     right_plucker_embedding_batch = batch['right']['plucker_embedding'] \n",
    "\n",
    "\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    if batch_idx >= 3:\n",
    "        break\n",
    "    video_name = batch['video_name'][0]     \n",
    "    print(f\"Current Video: {video_name}\")\n",
    "    left_data = batch['left']\n",
    "    right_data = batch['right']\n",
    "    first_left_rgb = left_data['pixel_values'][:, 0].to(device).to(dtype=weight_dtype)\n",
    "    last_left_rgb = left_data['pixel_values'][:, -1].to(device).to(dtype=weight_dtype)\n",
    "    right_events = right_data['events'].to(device).to(dtype=weight_dtype)\n",
    "    #print(f' first and last rgb {first_left_rgb.shape, last_left_rgb.shape}')\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        left_data['pixel_values'] = left_data['pixel_values'].to(device).to(dtype=weight_dtype)\n",
    "        right_events = right_events.to(device).to(dtype=weight_dtype)\n",
    "        evs_latents = right_events[:, :15]\n",
    "        evs_latents =  evs_latents.to(device)\n",
    "        noise =  torch.randn_like(last_left_rgb)\n",
    "\n",
    "        first_left_rgb_latent = first_left_rgb \n",
    "        conditions_latent = vae.encode(first_left_rgb_latent).latent_dist.mode()\n",
    "        conditions_latent = conditions_latent.unsqueeze(1).repeat(1, 15, 1, 1, 1)\n",
    "\n",
    "        conditions_ref = last_left_rgb\n",
    "        conditions_latent_ref = vae.encode(conditions_ref).latent_dist.mode()\n",
    "        conditions_latent_ref = conditions_latent_ref.unsqueeze(1).repeat(1,15, 1, 1, 1)\n",
    "        print(f'last_left_rgb_latent shape, after latent conversion is {conditions_latent_ref.shape}, first_left_rgb_latent shape, after latent conversion is {conditions_latent.shape}')\n",
    "\n",
    "        left_data['pixel_values'] = left_data['pixel_values'][:,:15]\n",
    "        pixel_values = rearrange(left_data['pixel_values'], \"b f c h w -> (b f) c h w\")\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "        #print(latents.shape)\n",
    "        latents = rearrange(latents, \"(b f) c h w -> b f c h w\", f=15)\n",
    "        print(f' latents shape is {latents.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
